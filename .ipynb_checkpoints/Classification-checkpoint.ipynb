{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Part 1: Cross-validation for training and testing\n",
    "\n",
    "So far we've used the same data for training and testing. That's definitely NOT a great idea. We'll now implement k-fold cross-validation. We'll pass k as a parameter, so we can decide how many folds we want to make. The general idea is that we need to split the data into subsections for training and for testing. Refer to your notes to remind yourself how cross-validation works.\n",
    "\n",
    "(a) Create a function called cross_validation_data(dataset, folds). This function is going to split our data in k folds, where k is the parameter given. The function should create (and ultimately return) a new list. We need to take a shallow copy of the data set, and operate on it. We need to determine how much data will be in each fold, by taking the length of our dataset, and dividing it by the number of folds, probably using integer division. \n",
    "\n",
    "Then we need to loop number of folds times, creating a new fold and populating that fold with data from our copy of the dataset. Roughly speaking that means:\n",
    "\n",
    "- while the amount of data in the current fold is less than the number we determined above for how much data SHOULD be in each fold\n",
    "    - choose a random instance from our copy of the data set\n",
    "        - HINT: You can use functions from the random library to help you (https://docs.python.org/3/library/random.html)\n",
    "    - Place the chosen instance into our current fold\n",
    "    - REMOVE the instance from the copy of the data\n",
    "    \n",
    "    - append the new fold to our list for returning\n",
    "- Continue until we've populated all the folds\n",
    "\n",
    "Before using whatever random method you choose to select values, we'll set the seed to 1. For this assignment it will mean that your results should be reproducably the same every time you run it, which can help with the testing. I've done that for you below.\n",
    "\n",
    "As an example, if we have a data set with 8 instances, and we split it into 4 folds, we'll have four sublists, each with two instances. E.g.\n",
    "if our input dataset was [[1,1], [2,1], [4,2], [6,1], [7,3], [8,4], [9,6], [5,2]]\n",
    "\n",
    "our output COULD be:\n",
    "\n",
    "[[[2,1], [4,2]], [[5,2], [8,4]], [[6,1],[9,6]], [[7,3], [1,1]]]\n",
    "\n",
    "NOTE the additional level of nesting, with respect to what happens in the function we write next. Create yourself a contrived test set, and try the function out. REMEMBER, that an instance can only appear in a single fold. \n",
    "\n",
    "This is a naive way to do this, and the resulting folds will definitely not be stratified, but it's better than anything we've done so far.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4, 2], [8, 4]], [[1, 1], [7, 3]], [[2, 1], [9, 6]], [[5, 2], [6, 1]]]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "from random import seed\n",
    "import copy\n",
    "\n",
    "seed(1)\n",
    "\n",
    "#(a)\n",
    "def cross_validation_data(dataset, folds):\n",
    "    new_list = []\n",
    "    copy_list = copy.deepcopy(dataset)\n",
    "    fold_len = len(dataset)/folds\n",
    "    for i in range(folds):\n",
    "        current_fold = []\n",
    "        while len(current_fold) < fold_len and len(copy_list)!=0:\n",
    "            random_inst = random.choice(copy_list) \n",
    "            current_fold.append(random_inst)\n",
    "            copy_list.remove(random_inst)\n",
    "        new_list.append(current_fold)\n",
    "    return new_list\n",
    "        \n",
    "#Test\n",
    "dataset = [[1,1], [2,1], [4,2], [6,1], [7,3], [8,4], [9,6], [5,2]]\n",
    "new_list = cross_validation_data(dataset, 4)\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Now you need to amend the evaluate algorithm function I've given you previously. I've changed the signature to include a folds parameter. This function needs to:\n",
    "\n",
    "- Use your function from (a) above, to create k folds of data\n",
    "- Create an empty list of scores\n",
    "- For each fold in your evaluation\n",
    "    - (1) set up the training set for that fold\n",
    "    - (2) You can think of that as REMOVING the fold which will be used for evaluation in this fold from the data you returned from (a)\n",
    "    - (3) BE CAREFUL to always operate on copies of the data - you don't want to mess up your original splits\n",
    "    - (4) You then need to 'flatten' the remaining data in the current training set\n",
    "        - (5) For example if the data was as given above:\n",
    "            - (6) [[[2,1], [4,2]], [[5,2], [8,4]], [[6,1],[9,6]], [[7,3], [1,1]]]\n",
    "        - (7) We would remove ONE fold first for testing (let's say the last one, but it will be each fold in turn)\n",
    "        - (8) Leaving [[[2,1], [4,2]], [[5,2], [8,4]], [[6,1],[9,6]]] for training\n",
    "        - (9) We need to convert that into: [[2,1], [4,2], [5,2], [8,4], [6,1],[9,6]]\n",
    "        - (10) This is the usual format we pass data into our algorithms\n",
    "    - (11) We also need to prepare our test set, which is the held-out fold from step (2) above\n",
    "    - (12) Append the instances from this held-out fold to a new list making sure the last value of each instance is NONE\n",
    "    - Once this is done, you should have a training set, comprised on k-1 folds of instances, and a test set, comprised of 1 fold of instances. We're now ready to evaluate our algorithm just as we have previously, using a training set, a test set and a specified evaluation metric. This should return a score to us. Instead of using that score directly, we should add it to our score list\n",
    "- At the end of this function, RETURN the complete list of scores. Therefore, if we did a 5 fold cross validation, we should get a list of 5 scores.\n",
    "\n",
    "I've given you the (very) bare bones of the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b\n",
    "def evaluate_algorithm(dataset, algorithm, folds, metric, *args):\n",
    "    new_data = cross_validation_data(dataset, folds)  \n",
    "    scores = []\n",
    "    for fold in new_data:\n",
    "        train = copy.deepcopy(new_data)\n",
    "        train.remove(fold)\n",
    "        train = [element for sublist in train for element in sublist]\n",
    "        test = [instance[:-1] + [None] for instance in fold]\n",
    "        \n",
    "        predicted = algorithm(train,test, *args)\n",
    "        actual = [instance[-1] for instance in fold]\n",
    "        result = metric(actual,predicted)\n",
    "        scores.append(result)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Applying cross-validation to real data\n",
    "\n",
    "To test the function you wrote above, let's apply it to the multivariate linear regression you wrote last week. Copy the function you wrote above to the cell below, along with all the code you need for BOTH MLR and zeroRR to work.\n",
    "Use the same parameters I gave you last week (a learning rate of 0.01 and 50 epochs of training), run MLR using a cross-validation of 5 folds. PRINT out the list of RMSE scores obtained on each fold. Then run zeroRR. \n",
    "Also print the LOWEST score obtained (that's the best), the highest score (that's the worst) and the mean RMSE score. See for yourself the variance in scores you can obtain using a cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 lrate =  0.01 error = 79.04791345235793\n",
      "Epoch = 1 lrate =  0.01 error = 66.81876975350525\n",
      "Epoch = 2 lrate =  0.01 error = 65.26701394512152\n",
      "Epoch = 3 lrate =  0.01 error = 64.42742764601059\n",
      "Epoch = 4 lrate =  0.01 error = 63.93708027599893\n",
      "Epoch = 5 lrate =  0.01 error = 63.63938714532394\n",
      "Epoch = 6 lrate =  0.01 error = 63.4524838985086\n",
      "Epoch = 7 lrate =  0.01 error = 63.33143057910085\n",
      "Epoch = 8 lrate =  0.01 error = 63.25065408311598\n",
      "Epoch = 9 lrate =  0.01 error = 63.19512696352972\n",
      "Epoch = 10 lrate =  0.01 error = 63.155761759429296\n",
      "Epoch = 11 lrate =  0.01 error = 63.126924265661906\n",
      "Epoch = 12 lrate =  0.01 error = 63.105046781197196\n",
      "Epoch = 13 lrate =  0.01 error = 63.087830340997094\n",
      "Epoch = 14 lrate =  0.01 error = 63.073772226819536\n",
      "Epoch = 15 lrate =  0.01 error = 63.06187868255402\n",
      "Epoch = 16 lrate =  0.01 error = 63.05148619484045\n",
      "Epoch = 17 lrate =  0.01 error = 63.04214814046143\n",
      "Epoch = 18 lrate =  0.01 error = 63.03356173602727\n",
      "Epoch = 19 lrate =  0.01 error = 63.02552034751806\n",
      "Epoch = 20 lrate =  0.01 error = 63.01788203108742\n",
      "Epoch = 21 lrate =  0.01 error = 63.01054860811781\n",
      "Epoch = 22 lrate =  0.01 error = 63.00345165428858\n",
      "Epoch = 23 lrate =  0.01 error = 62.99654306744649\n",
      "Epoch = 24 lrate =  0.01 error = 62.98978868950846\n",
      "Epoch = 25 lrate =  0.01 error = 62.983163976998846\n",
      "Epoch = 26 lrate =  0.01 error = 62.97665105208741\n",
      "Epoch = 27 lrate =  0.01 error = 62.97023668736021\n",
      "Epoch = 28 lrate =  0.01 error = 62.963910924088566\n",
      "Epoch = 29 lrate =  0.01 error = 62.957666121451844\n",
      "Epoch = 30 lrate =  0.01 error = 62.951496299631\n",
      "Epoch = 31 lrate =  0.01 error = 62.94539668376415\n",
      "Epoch = 32 lrate =  0.01 error = 62.939363385510156\n",
      "Epoch = 33 lrate =  0.01 error = 62.933393179137106\n",
      "Epoch = 34 lrate =  0.01 error = 62.92748334273645\n",
      "Epoch = 35 lrate =  0.01 error = 62.921631544461334\n",
      "Epoch = 36 lrate =  0.01 error = 62.915835760035044\n",
      "Epoch = 37 lrate =  0.01 error = 62.91009421209375\n",
      "Epoch = 38 lrate =  0.01 error = 62.90440532487872\n",
      "Epoch = 39 lrate =  0.01 error = 62.89876768980308\n",
      "Epoch = 40 lrate =  0.01 error = 62.89318003881457\n",
      "Epoch = 41 lrate =  0.01 error = 62.88764122339837\n",
      "Epoch = 42 lrate =  0.01 error = 62.88215019773109\n",
      "Epoch = 43 lrate =  0.01 error = 62.8767060049319\n",
      "Epoch = 44 lrate =  0.01 error = 62.87130776566861\n",
      "Epoch = 45 lrate =  0.01 error = 62.865954668584216\n",
      "Epoch = 46 lrate =  0.01 error = 62.86064596216064\n",
      "Epoch = 47 lrate =  0.01 error = 62.85538094773833\n",
      "Epoch = 48 lrate =  0.01 error = 62.85015897347574\n",
      "Epoch = 49 lrate =  0.01 error = 62.84497942909383\n",
      "Epoch = 0 lrate =  0.01 error = 79.71363468188812\n",
      "Epoch = 1 lrate =  0.01 error = 67.14786284983761\n",
      "Epoch = 2 lrate =  0.01 error = 65.37184244929054\n",
      "Epoch = 3 lrate =  0.01 error = 64.41654367711595\n",
      "Epoch = 4 lrate =  0.01 error = 63.859842331871185\n",
      "Epoch = 5 lrate =  0.01 error = 63.521651266994006\n",
      "Epoch = 6 lrate =  0.01 error = 63.30867017898215\n",
      "Epoch = 7 lrate =  0.01 error = 63.17001260597696\n",
      "Epoch = 8 lrate =  0.01 error = 63.076866539709656\n",
      "Epoch = 9 lrate =  0.01 error = 63.01236231096365\n",
      "Epoch = 10 lrate =  0.01 error = 62.96631837454377\n",
      "Epoch = 11 lrate =  0.01 error = 62.932422414187634\n",
      "Epoch = 12 lrate =  0.01 error = 62.906668884844954\n",
      "Epoch = 13 lrate =  0.01 error = 62.886465152623686\n",
      "Epoch = 14 lrate =  0.01 error = 62.87010460507307\n",
      "Epoch = 15 lrate =  0.01 error = 62.85644747611745\n",
      "Epoch = 16 lrate =  0.01 error = 62.84472278667405\n",
      "Epoch = 17 lrate =  0.01 error = 62.83440288102374\n",
      "Epoch = 18 lrate =  0.01 error = 62.82512256376649\n",
      "Epoch = 19 lrate =  0.01 error = 62.81662623023274\n",
      "Epoch = 20 lrate =  0.01 error = 62.808732887175616\n",
      "Epoch = 21 lrate =  0.01 error = 62.80131277961602\n",
      "Epoch = 22 lrate =  0.01 error = 62.794271640452365\n",
      "Epoch = 23 lrate =  0.01 error = 62.78753999756454\n",
      "Epoch = 24 lrate =  0.01 error = 62.78106586468257\n",
      "Epoch = 25 lrate =  0.01 error = 62.77480971227754\n",
      "Epoch = 26 lrate =  0.01 error = 62.768740984190394\n",
      "Epoch = 27 lrate =  0.01 error = 62.76283566799949\n",
      "Epoch = 28 lrate =  0.01 error = 62.757074587449296\n",
      "Epoch = 29 lrate =  0.01 error = 62.75144219220123\n",
      "Epoch = 30 lrate =  0.01 error = 62.7459256919272\n",
      "Epoch = 31 lrate =  0.01 error = 62.74051443017318\n",
      "Epoch = 32 lrate =  0.01 error = 62.735199426225996\n",
      "Epoch = 33 lrate =  0.01 error = 62.72997303553206\n",
      "Epoch = 34 lrate =  0.01 error = 62.724828694435224\n",
      "Epoch = 35 lrate =  0.01 error = 62.71976072545428\n",
      "Epoch = 36 lrate =  0.01 error = 62.71476418645942\n",
      "Epoch = 37 lrate =  0.01 error = 62.70983475206476\n",
      "Epoch = 38 lrate =  0.01 error = 62.70496861896111\n",
      "Epoch = 39 lrate =  0.01 error = 62.70016242929165\n",
      "Epoch = 40 lrate =  0.01 error = 62.695413207813765\n",
      "Epoch = 41 lrate =  0.01 error = 62.69071830976745\n",
      "Epoch = 42 lrate =  0.01 error = 62.686075377169566\n",
      "Epoch = 43 lrate =  0.01 error = 62.6814823018444\n",
      "Epoch = 44 lrate =  0.01 error = 62.676937193914654\n",
      "Epoch = 45 lrate =  0.01 error = 62.67243835476659\n",
      "Epoch = 46 lrate =  0.01 error = 62.66798425373471\n",
      "Epoch = 47 lrate =  0.01 error = 62.66357350789994\n",
      "Epoch = 48 lrate =  0.01 error = 62.659204864526366\n",
      "Epoch = 49 lrate =  0.01 error = 62.65487718573925\n",
      "Epoch = 0 lrate =  0.01 error = 79.4466129082375\n",
      "Epoch = 1 lrate =  0.01 error = 67.09952720894732\n",
      "Epoch = 2 lrate =  0.01 error = 65.47172147569405\n",
      "Epoch = 3 lrate =  0.01 error = 64.61161250967307\n",
      "Epoch = 4 lrate =  0.01 error = 64.11859486605351\n",
      "Epoch = 5 lrate =  0.01 error = 63.824147255190624\n",
      "Epoch = 6 lrate =  0.01 error = 63.64169894514149\n",
      "Epoch = 7 lrate =  0.01 error = 63.52460725341768\n",
      "Epoch = 8 lrate =  0.01 error = 63.44685218449762\n",
      "Epoch = 9 lrate =  0.01 error = 63.39344209348123\n",
      "Epoch = 10 lrate =  0.01 error = 63.3554753899309\n",
      "Epoch = 11 lrate =  0.01 error = 63.32752015802142\n",
      "Epoch = 12 lrate =  0.01 error = 63.30618098997673\n",
      "Epoch = 13 lrate =  0.01 error = 63.2892910255608\n",
      "Epoch = 14 lrate =  0.01 error = 63.27544282200505\n",
      "Epoch = 15 lrate =  0.01 error = 63.26370833132476\n",
      "Epoch = 16 lrate =  0.01 error = 63.25346754365413\n",
      "Epoch = 17 lrate =  0.01 error = 63.244301331293386\n",
      "Epoch = 18 lrate =  0.01 error = 63.23592319939598\n",
      "Epoch = 19 lrate =  0.01 error = 63.228135149323435\n",
      "Epoch = 20 lrate =  0.01 error = 63.22079877540536\n",
      "Epoch = 21 lrate =  0.01 error = 63.21381614093585\n",
      "Epoch = 22 lrate =  0.01 error = 63.207117015227276\n",
      "Epoch = 23 lrate =  0.01 error = 63.200650292913934\n",
      "Epoch = 24 lrate =  0.01 error = 63.19437818710899\n",
      "Epoch = 25 lrate =  0.01 error = 63.18827227556847\n",
      "Epoch = 26 lrate =  0.01 error = 63.18231079219441\n",
      "Epoch = 27 lrate =  0.01 error = 63.17647675986864\n",
      "Epoch = 28 lrate =  0.01 error = 63.170756694368976\n",
      "Epoch = 29 lrate =  0.01 error = 63.16513969768085\n",
      "Epoch = 30 lrate =  0.01 error = 63.159616818041\n",
      "Epoch = 31 lrate =  0.01 error = 63.1541805935785\n",
      "Epoch = 32 lrate =  0.01 error = 63.14882472302071\n",
      "Epoch = 33 lrate =  0.01 error = 63.14354382488776\n",
      "Epoch = 34 lrate =  0.01 error = 63.138333258766004\n",
      "Epoch = 35 lrate =  0.01 error = 63.13318899050449\n",
      "Epoch = 36 lrate =  0.01 error = 63.12810748880635\n",
      "Epoch = 37 lrate =  0.01 error = 63.12308564452178\n",
      "Epoch = 38 lrate =  0.01 error = 63.11812070657968\n",
      "Epoch = 39 lrate =  0.01 error = 63.11321023029268\n",
      "Epoch = 40 lrate =  0.01 error = 63.108352035016445\n",
      "Epoch = 41 lrate =  0.01 error = 63.10354416900189\n",
      "Epoch = 42 lrate =  0.01 error = 63.09878487986773\n",
      "Epoch = 43 lrate =  0.01 error = 63.0940725895505\n",
      "Epoch = 44 lrate =  0.01 error = 63.08940587287034\n",
      "Epoch = 45 lrate =  0.01 error = 63.084783439064864\n",
      "Epoch = 46 lrate =  0.01 error = 63.080204115791744\n",
      "Epoch = 47 lrate =  0.01 error = 63.0756668352058\n",
      "Epoch = 48 lrate =  0.01 error = 63.071170621801755\n",
      "Epoch = 49 lrate =  0.01 error = 63.066714581766355\n",
      "Epoch = 0 lrate =  0.01 error = 78.76561408525016\n",
      "Epoch = 1 lrate =  0.01 error = 66.33608006771766\n",
      "Epoch = 2 lrate =  0.01 error = 64.66668253862387\n",
      "Epoch = 3 lrate =  0.01 error = 63.75854478363197\n",
      "Epoch = 4 lrate =  0.01 error = 63.22232122767824\n",
      "Epoch = 5 lrate =  0.01 error = 62.89319652435584\n",
      "Epoch = 6 lrate =  0.01 error = 62.68441063920746\n",
      "Epoch = 7 lrate =  0.01 error = 62.547920724106994\n",
      "Epoch = 8 lrate =  0.01 error = 62.456142161737425\n",
      "Epoch = 9 lrate =  0.01 error = 62.39271298632673\n",
      "Epoch = 10 lrate =  0.01 error = 62.34764094486964\n",
      "Epoch = 11 lrate =  0.01 error = 62.31466701439049\n",
      "Epoch = 12 lrate =  0.01 error = 62.28978565536697\n",
      "Epoch = 13 lrate =  0.01 error = 62.27038760521717\n",
      "Epoch = 14 lrate =  0.01 error = 62.254748324310654\n",
      "Epoch = 15 lrate =  0.01 error = 62.2417144071611\n",
      "Epoch = 16 lrate =  0.01 error = 62.230506801996036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 lrate =  0.01 error = 62.2205948703652\n",
      "Epoch = 18 lrate =  0.01 error = 62.211614466185736\n",
      "Epoch = 19 lrate =  0.01 error = 62.20331394357495\n",
      "Epoch = 20 lrate =  0.01 error = 62.195518194424814\n",
      "Epoch = 21 lrate =  0.01 error = 62.18810449107337\n",
      "Epoch = 22 lrate =  0.01 error = 62.18098614694499\n",
      "Epoch = 23 lrate =  0.01 error = 62.17410140202331\n",
      "Epoch = 24 lrate =  0.01 error = 62.16740582566913\n",
      "Epoch = 25 lrate =  0.01 error = 62.16086710126965\n",
      "Epoch = 26 lrate =  0.01 error = 62.15446143157894\n",
      "Epoch = 27 lrate =  0.01 error = 62.148171051320254\n",
      "Epoch = 28 lrate =  0.01 error = 62.141982498947876\n",
      "Epoch = 29 lrate =  0.01 error = 62.13588541058862\n",
      "Epoch = 30 lrate =  0.01 error = 62.129871674249934\n",
      "Epoch = 31 lrate =  0.01 error = 62.12393483337197\n",
      "Epoch = 32 lrate =  0.01 error = 62.11806966350728\n",
      "Epoch = 33 lrate =  0.01 error = 62.11227186964607\n",
      "Epoch = 34 lrate =  0.01 error = 62.10653786794795\n",
      "Epoch = 35 lrate =  0.01 error = 62.10086462681036\n",
      "Epoch = 36 lrate =  0.01 error = 62.09524954986656\n",
      "Epoch = 37 lrate =  0.01 error = 62.08969038880741\n",
      "Epoch = 38 lrate =  0.01 error = 62.0841851775689\n",
      "Epoch = 39 lrate =  0.01 error = 62.07873218195332\n",
      "Epoch = 40 lrate =  0.01 error = 62.07332986051446\n",
      "Epoch = 41 lrate =  0.01 error = 62.06797683374673\n",
      "Epoch = 42 lrate =  0.01 error = 62.06267185947233\n",
      "Epoch = 43 lrate =  0.01 error = 62.05741381291509\n",
      "Epoch = 44 lrate =  0.01 error = 62.05220167036738\n",
      "Epoch = 45 lrate =  0.01 error = 62.04703449564679\n",
      "Epoch = 46 lrate =  0.01 error = 62.04191142875556\n",
      "Epoch = 47 lrate =  0.01 error = 62.03683167629816\n",
      "Epoch = 48 lrate =  0.01 error = 62.031794503323844\n",
      "Epoch = 49 lrate =  0.01 error = 62.02679922633346\n",
      "Epoch = 0 lrate =  0.01 error = 79.16760974912555\n",
      "Epoch = 1 lrate =  0.01 error = 66.75656282383723\n",
      "Epoch = 2 lrate =  0.01 error = 65.04345578712704\n",
      "Epoch = 3 lrate =  0.01 error = 64.12978989040047\n",
      "Epoch = 4 lrate =  0.01 error = 63.60083578047653\n",
      "Epoch = 5 lrate =  0.01 error = 63.281724684821654\n",
      "Epoch = 6 lrate =  0.01 error = 63.08226878535247\n",
      "Epoch = 7 lrate =  0.01 error = 62.95345863265499\n",
      "Epoch = 8 lrate =  0.01 error = 62.8676884871245\n",
      "Epoch = 9 lrate =  0.01 error = 62.80889714462302\n",
      "Epoch = 10 lrate =  0.01 error = 62.767452112396874\n",
      "Epoch = 11 lrate =  0.01 error = 62.73741232913647\n",
      "Epoch = 12 lrate =  0.01 error = 62.71502023226001\n",
      "Epoch = 13 lrate =  0.01 error = 62.69784640671888\n",
      "Epoch = 14 lrate =  0.01 error = 62.684290097698145\n",
      "Epoch = 15 lrate =  0.01 error = 62.67327926930082\n",
      "Epoch = 16 lrate =  0.01 error = 62.664085774839876\n",
      "Epoch = 17 lrate =  0.01 error = 62.65620880581762\n",
      "Epoch = 18 lrate =  0.01 error = 62.64929991745954\n",
      "Epoch = 19 lrate =  0.01 error = 62.6431139806504\n",
      "Epoch = 20 lrate =  0.01 error = 62.63747664130983\n",
      "Epoch = 21 lrate =  0.01 error = 62.632262478051345\n",
      "Epoch = 22 lrate =  0.01 error = 62.62738019652722\n",
      "Epoch = 23 lrate =  0.01 error = 62.62276250880832\n",
      "Epoch = 24 lrate =  0.01 error = 62.61835916350136\n",
      "Epoch = 25 lrate =  0.01 error = 62.61413211244307\n",
      "Epoch = 26 lrate =  0.01 error = 62.610052136484654\n",
      "Epoch = 27 lrate =  0.01 error = 62.6060964738143\n",
      "Epoch = 28 lrate =  0.01 error = 62.602247140965126\n",
      "Epoch = 29 lrate =  0.01 error = 62.59848973495665\n",
      "Epoch = 30 lrate =  0.01 error = 62.594812571387315\n",
      "Epoch = 31 lrate =  0.01 error = 62.59120605839177\n",
      "Epoch = 32 lrate =  0.01 error = 62.58766223717644\n",
      "Epoch = 33 lrate =  0.01 error = 62.58417444095912\n",
      "Epoch = 34 lrate =  0.01 error = 62.580737038684475\n",
      "Epoch = 35 lrate =  0.01 error = 62.57734523994199\n",
      "Epoch = 36 lrate =  0.01 error = 62.57399494446188\n",
      "Epoch = 37 lrate =  0.01 error = 62.570682624421146\n",
      "Epoch = 38 lrate =  0.01 error = 62.567405231162645\n",
      "Epoch = 39 lrate =  0.01 error = 62.56416012030863\n",
      "Epoch = 40 lrate =  0.01 error = 62.56094499089922\n",
      "Epoch = 41 lrate =  0.01 error = 62.55775783537678\n",
      "Epoch = 42 lrate =  0.01 error = 62.55459689806828\n",
      "Epoch = 43 lrate =  0.01 error = 62.55146064040859\n",
      "Epoch = 44 lrate =  0.01 error = 62.548347711588434\n",
      "Epoch = 45 lrate =  0.01 error = 62.545256923612946\n",
      "Epoch = 46 lrate =  0.01 error = 62.542187229987206\n",
      "Epoch = 47 lrate =  0.01 error = 62.539137707418085\n",
      "Epoch = 48 lrate =  0.01 error = 62.53610754004318\n",
      "Epoch = 49 lrate =  0.01 error = 62.53309600578968\n",
      "MLR RMSE's list: [0.12484980973634346, 0.1256942914754814, 0.12445321922104036, 0.12814966781623083, 0.13187853230400415]\n",
      "MLR highest RMSE: 0.13187853230400415\n",
      "MLR lowest RMSE: 0.12445321922104036\n",
      "MLR mean RMSE: 0.12700510411062002 \n",
      "\n",
      "zeroRR RMSE's list: [0.14801668771789225, 0.14207553871360873, 0.14886060533915377, 0.151222312150378, 0.14776248561341673]\n",
      "zeroRR highest RMSE: 0.151222312150378\n",
      "zeroRR lowest RMSE: 0.14207553871360873\n",
      "zeroRR avg RMSE: 0.1475875259068899\n"
     ]
    }
   ],
   "source": [
    "def evaluate_algorithm(dataset, algorithm, folds, metric, *args):\n",
    "    new_data = cross_validation_data(dataset, folds)  \n",
    "    scores = []\n",
    "    for fold in new_data:\n",
    "        train = copy.deepcopy(new_data)\n",
    "        train.remove(fold)\n",
    "        train = [element for sublist in train for element in sublist]\n",
    "        test = [instance[:-1] + [None] for instance in fold]\n",
    "        \n",
    "        predicted = algorithm(train,test, *args)\n",
    "        actual = [instance[-1] for instance in fold]           \n",
    "        result = metric(actual,predicted)\n",
    "        scores.append(result)\n",
    "\n",
    "    return scores\n",
    "\n",
    "#================ MLR - ZeroRR ==================\n",
    "#================================================\n",
    "def predict(instance, coefficients):\n",
    "    output = coefficients[0]\n",
    "    for i in range(len(instance)-1):\n",
    "        output += instance[i] * coefficients[i+1]\n",
    "    return output\n",
    "\n",
    "def coefficientsSGD(train, learning_rate, epochs):\n",
    "    length = len(train[0])\n",
    "    coefficients = [0.0 for i in range(length)]\n",
    "    for i in range(epochs):\n",
    "        total_error = 0\n",
    "        for instance in train:\n",
    "            prediction = predict(instance, coefficients)\n",
    "            error = prediction - instance[len(instance)-1]\n",
    "            total_error += error**2\n",
    "            \n",
    "            #update coefficients\n",
    "            coefficients[0] = coefficients[0] - learning_rate * error\n",
    "            for index in range(length-1):\n",
    "                coefficients[index+1] = coefficients[index+1] - learning_rate * error * instance[index]\n",
    "        print(\"Epoch =\", i, \"lrate = \", learning_rate, \"error =\", total_error)\n",
    "    return coefficients\n",
    "            \n",
    "def mlr(train,test,learning_rate,epochs):\n",
    "    prediction = []\n",
    "    coeff = coefficientsSGD(train, learning_rate, epochs)\n",
    "    for instance in test:\n",
    "        prediction.append(predict(instance, coeff))\n",
    "    return prediction\n",
    "\n",
    "def rmse_eval(actual, predicted):\n",
    "    error = 0.0\n",
    "    length = len(actual)\n",
    "    for i in range(length):\n",
    "        error += (predicted[i] - actual[i])**2\n",
    "    error = (error/length)**0.5\n",
    "    return error\n",
    "\n",
    "def mean(listOfValues):\n",
    "    return sum(listOfValues)/len(listOfValues)\n",
    "\n",
    "def zeroRR(train, test):\n",
    "    targetList = [instance[len(instance)-1] for instance in train]\n",
    "    targetMean = mean(targetList)\n",
    "    prediction = [targetMean for i in range(len(train))]\n",
    "    return prediction\n",
    "\n",
    "#=======================================================\n",
    "import csv\n",
    "\n",
    "def load_data(filename):\n",
    "    csv_reader = csv.reader(open(filename, newline=''), delimiter=',')\n",
    "    new_list = []\n",
    "    for row in csv_reader:\n",
    "        new_list.append(row)\n",
    "    return new_list\n",
    "\n",
    "data_wine = load_data(\"winequality-white.csv\")\n",
    "\n",
    "# (g) Convert the features from strings to floats \n",
    "def column2Float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column])\n",
    "\n",
    "for i in range(len(data_wine[0])):  \n",
    "    column2Float(data_wine, i)\n",
    "    \n",
    "# (h) Normalize all the attributes\n",
    "def minmax(dataset):\n",
    "    col_len = len(dataset[0])\n",
    "    min_max = []\n",
    "    for i in range(col_len):\n",
    "        col = [row[i] for row in dataset]\n",
    "        min_max.append([min(col), max(col)])\n",
    "    return min_max\n",
    "\n",
    "def normalize(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0])/(minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "normalize(data_wine, minmax(data_wine))\n",
    "\n",
    "#==============================================\n",
    "dataset = data_wine\n",
    "learning_rate = 0.01\n",
    "epochs = 50 \n",
    "mlr_result = evaluate_algorithm(dataset,mlr,5,rmse_eval,learning_rate,epochs)\n",
    "zeroRR_result = evaluate_algorithm(dataset,zeroRR,5,rmse_eval)\n",
    "\n",
    "print(\"MLR RMSE's list:\", mlr_result)\n",
    "print(\"MLR highest RMSE:\", max(mlr_result))\n",
    "print(\"MLR lowest RMSE:\", min(mlr_result))\n",
    "print(\"MLR mean RMSE:\", mean(mlr_result), \"\\n\")\n",
    "\n",
    "print(\"zeroRR RMSE's list:\", zeroRR_result)\n",
    "print(\"zeroRR highest RMSE:\", max(zeroRR_result))\n",
    "print(\"zeroRR lowest RMSE:\", min(zeroRR_result))\n",
    "print(\"zeroRR avg RMSE:\", mean(zeroRR_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Classification with Logistic Regression\n",
    "\n",
    "Everything so far has been a regression task - predicting a numeric value. We've moved on to talk about classification in class, so let's implement our first basic classifier. This is the same idea as linear regression, but we're going to predict one of two binary classes, using logistic regression.\n",
    "\n",
    "The general outline for logistic regression is the same as for multivariate linear regression. We're going to need a function to make predictions, and a function to learn coefficients. \n",
    "\n",
    "(a) The formula for making a prediction, predY, for logistic regression is:\n",
    "\n",
    "predY = 1.0 / 1.0 + e^-(b0 + b1 * x1 + ... + bN * xN)\n",
    "\n",
    "Where b0 is the intercept or bias, bN is the coefficient for the input variable xN, and e is the base of the natural logarithms, or Euler's number. We can use the python math library which has an implementation of e called math.exp(x): https://docs.python.org/3/library/math.html\n",
    "\n",
    "The formula given above is an implementation of a sigmoid function (a commonly used, s-shaped function that can take any input value and produce a number between 0 and 1).\n",
    "\n",
    "We will assume there can be multiple input features (x1, x2 etc) not just a single value, and that each input feature will have a corresponding coefficient (b1, b2 etc).\n",
    "\n",
    "Write your predict function, that will take an instance, and a list of coefficients, and return a prediction. In the list of coefficients, assume coefficient[0] corresponds to b0. This will be very similar to your predict function from last week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your predict function here\n",
    "import math\n",
    "from math import exp\n",
    "\n",
    "def logistic_predict(instance, coefficients):\n",
    "    sum_beta = coefficients[0]\n",
    "    for i in range(len(instance)-1):\n",
    "        sum_beta += instance[i] * coefficients[i+1]\n",
    "    predY = 1.0/(1.0 + math.exp(-(sum_beta)))\n",
    "    return predY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test your predict function on the contrived dataset below. It includes TWO input variables and a class (Y) for each instance. The class is either 0 or 1.\n",
    "\n",
    "(b) Graph this data, x1 against x2, using different colored points for the two classes. Include a legend, showing which color corresponds to which class. \n",
    "\n",
    "(c) Call your predict function on each instance in the contrived data set, using the coefficient list given below. Get the predicted class from your function, and print (for each instance), the expected class, the predicted value AND the predicted class. In order to get the predicted class from the value predicted, we need to do rounding. There is a round() function that can help you. If it works correctly, you should predict the correct class of each instance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected class: 0\n",
      "Predicted value: 0.2987569855650975\n",
      "Predicted class: 0 \n",
      "\n",
      "Expected class: 0\n",
      "Predicted value: 0.14595105593031163\n",
      "Predicted class: 0 \n",
      "\n",
      "Expected class: 0\n",
      "Predicted value: 0.08533326519733725\n",
      "Predicted class: 0 \n",
      "\n",
      "Expected class: 0\n",
      "Predicted value: 0.21973731424800344\n",
      "Predicted class: 0 \n",
      "\n",
      "Expected class: 0\n",
      "Predicted value: 0.24705900008926596\n",
      "Predicted class: 0 \n",
      "\n",
      "Expected class: 1\n",
      "Predicted value: 0.9547021347460022\n",
      "Predicted class: 1 \n",
      "\n",
      "Expected class: 1\n",
      "Predicted value: 0.8620341905282771\n",
      "Predicted class: 1 \n",
      "\n",
      "Expected class: 1\n",
      "Predicted value: 0.9717729050420985\n",
      "Predicted class: 1 \n",
      "\n",
      "Expected class: 1\n",
      "Predicted value: 0.9992954520878627\n",
      "Predicted class: 1 \n",
      "\n",
      "Expected class: 1\n",
      "Predicted value: 0.9054893228110497\n",
      "Predicted class: 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's the contrived data set\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "\n",
    "# Do the graphing here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1val = [pair[0] for pair in dataset]\n",
    "x2val = [pair[1] for pair in dataset]\n",
    "yval = [pair[2] for pair in dataset]\n",
    "plt.plot(x1val, yval, '^', label=\"x1\")\n",
    "plt.plot(x2val, yval, 'r^', label=\"x2\")\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Call your predict function on the data here, using the following coefficients\n",
    "coeffs = [-0.406605464, 0.852573316, -1.104746259]\n",
    "\n",
    "for instance in dataset:\n",
    "    predY = logistic_predict(instance, coeffs)\n",
    "    print(\"Expected class:\", instance[-1])\n",
    "    print(\"Predicted value:\", predY)\n",
    "    print(\"Predicted class:\", round(predY), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Above I gave you coefficients. Just as with MLR, we need to estimate the coefficients for a data set. To do that, we're going to use stochastic gradient descent. The algorithm is exactly the same as for multivariate linear regression except for the following two things.\n",
    "\n",
    "b0 is computed by:\n",
    "\n",
    "b0 = b0 + learning_rate * error * predictedY * (1.0 - predictedY)\n",
    "\n",
    "and bN is computed by:\n",
    "\n",
    "bN = bN + learning_rate * error * predictedY * (1.0 - predictedY) * xN\n",
    "\n",
    "for all coefficients b1..bN\n",
    "\n",
    "Remember, to calculate the error, we run the algorithm with default coefficients and perform prediction, then get the error by subtracting the predictedY from the actual Y value.\n",
    "\n",
    "Refer back to Assignment 4 for the complete algorithm\n",
    "\n",
    "(d) Apply your function to the contrived dataset given above, using the learning rate of 0.3, and 100 epochs. Print the resulting coefficients. I've shown my last 5 epochs of this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients are: [-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n"
     ]
    }
   ],
   "source": [
    "# Write your function sgd_log(dataset, learning_rate, epochs) here\n",
    "def sgd_log(dataset, learning_rate, epochs):\n",
    "    length = len(dataset[0])\n",
    "    coefficients = [0.0 for i in range(length)]\n",
    "    for i in range(epochs):\n",
    "        total_error = 0\n",
    "        for instance in dataset:\n",
    "            predictedY = logistic_predict(instance, coefficients)\n",
    "            error = instance[-1] - predictedY \n",
    "            total_error += error**2\n",
    "            \n",
    "            #update coefficients\n",
    "            coefficients[0] = coefficients[0] + learning_rate * error * predictedY * (1.0 - predictedY)\n",
    "            for index in range(length-1):\n",
    "                coefficients[index+1] = coefficients[index+1] + learning_rate * error * predictedY * (1.0 - predictedY) * instance[index]\n",
    "        #print(\"Epoch =\", i, \"lrate = \", learning_rate, \"error =\", total_error)\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "# Call your function using the parameters given here. \n",
    "learning_rate = 0.3\n",
    "epochs = 100\n",
    "coefs = sgd_log(dataset, learning_rate, epochs)\n",
    "print(\"Coefficients are:\", coefs)\n",
    "\n",
    "\n",
    "\n",
    "# Example output\n",
    "#\n",
    "#>epoch=95, lrate=0.300, error=0.023\n",
    "#>epoch=96, lrate=0.300, error=0.023\n",
    "#>epoch=97, lrate=0.300, error=0.023\n",
    "#>epoch=98, lrate=0.300, error=0.023\n",
    "#>epoch=99, lrate=0.300, error=0.022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Applying classification to real data\n",
    "\n",
    "In this final section, we'll do the following things. \n",
    "\n",
    "(a) We need a function for calculating accuracy. It will take a list of actual class values, and predicted class values. If the actual value of an instance and the predicted value of an instance are the same, increment a counter. In the end, return the value of this counter divided by the length of the actual values list, multiplied by 100 - so we are returning a percentage of the classification we got correct. This function should be called accuracy(actual, predicted).\n",
    "\n",
    "(b) We need a baseline function. Create a function called zeroRC(train, test). This function should take in the training data, and find the most common value of Y in the training data. It should then return a list of predictions the same length as the test data, containing ONLY this value that was most common in the training data. \n",
    "\n",
    "(c) I've given you the diabetes data set. You can find more about this data set here: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "You are going to:\n",
    "\n",
    "- load the data\n",
    "- print out some basic information about the data (number of instances, features)\n",
    "- convert each string value to float (for all columns)\n",
    "- normalize all columns in the range 0-1\n",
    "- perform a 5-fold cross validation\n",
    "    - using logistic regression\n",
    "    - using a learning rate of 0.1, and 100 epochs\n",
    "- collect predicted scores\n",
    "- print the min, max and mean scores\n",
    "- repeat the above, using zeroRC as the algorithm\n",
    "- offer me some write up of the results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 768\n",
      "Number of features : 8 \n",
      "\n",
      "\n",
      "Logistic Reg highest: 79.22077922077922\n",
      "Logistic Reg lowest: 74.02597402597402\n",
      "Logistic Reg mean: 76.69002050580997 \n",
      "\n",
      "zeroRC highest: 69.48051948051948\n",
      "zeroRC lowest: 62.98701298701299\n",
      "zeroRC RMSE's avg: 65.10252904989747\n"
     ]
    }
   ],
   "source": [
    "# Do all the code here\n",
    "#a\n",
    "def accuracy(actual, predicted):\n",
    "    length = len(actual)\n",
    "    counter = 0\n",
    "    for i in range(length):\n",
    "        if actual[i] == predicted[i]:\n",
    "            counter += 1\n",
    "    return (counter/length)*100\n",
    "\n",
    "#b\n",
    "import collections\n",
    "from collections import Counter \n",
    "\n",
    "def zeroRC(train, test):\n",
    "    valueY = [instance[-1] for instance in train]\n",
    "    most_occur = Counter(valueY).most_common(1)[0][0] \n",
    "    return [most_occur for i in range(len(test))]\n",
    "\n",
    "#c\n",
    "def logistic_reg(train,test,learning_rate,epochs):\n",
    "    prediction = []\n",
    "    coeff = sgd_log(train, learning_rate, epochs)\n",
    "    for instance in test:\n",
    "        prediction.append(round(logistic_predict(instance, coeff)))\n",
    "    return prediction\n",
    "\n",
    "data_diabetes = load_data(\"pima-indians-diabetes.csv\")\n",
    "print(\"Number of instances:\", len(data_diabetes))\n",
    "print(\"Number of features :\", len(data_diabetes[0])-1, \"\\n\")\n",
    "\n",
    "for i in range(len(data_diabetes[0])):  \n",
    "    column2Float(data_diabetes, i)\n",
    "    \n",
    "normalize(data_diabetes, minmax(data_diabetes))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "lr_result = evaluate_algorithm(data_diabetes,logistic_reg,5,accuracy,learning_rate,epochs)\n",
    "zeroRC_result = evaluate_algorithm(data_diabetes,zeroRC,5,accuracy)\n",
    "\n",
    "print(\"\\nLogistic Reg highest:\", max(lr_result))\n",
    "print(\"Logistic Reg lowest:\", min(lr_result))\n",
    "print(\"Logistic Reg mean:\", mean(lr_result), \"\\n\")\n",
    "\n",
    "print(\"zeroRC highest:\", max(zeroRC_result))\n",
    "print(\"zeroRC lowest:\", min(zeroRC_result))\n",
    "print(\"zeroRC RMSE's avg:\", mean(zeroRC_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write up your observations on the experiment here\n",
    "\n",
    "Logistic Regression outperforms zeroRC in all aspects (min/max/mean scores). By using a 5-fold cross validation instead of the same dataset for train and test, we are more confident that the RMSE scores reflect better the performance of the two algorithms.\n",
    "\n",
    "About the coefficients obtained from logistic regression:\n",
    " - The signs are pretty consistent among the 5 folds (except for SkinThickness and Age change sign twice and once respectively).\n",
    " - Overall, Pregnancies, Glucose, BMIBody mass index, DiabetesPedigreeFunction, and Age have positive effect to diabetes while BloodPressure, SkinThickness, and Insulin have negative effect.\n",
    " - Glucose has the highest effect on the predicted output: a 0.1 unit increase in Glucose is related to around 0.6 unit increase in the output => higher glucose is related to higher chance of diabetes.\n",
    " - Some features have large variance in the coefficients across the 5 folds (such as Age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
