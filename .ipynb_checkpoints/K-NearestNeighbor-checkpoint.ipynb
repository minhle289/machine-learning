{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor\n",
    "\n",
    "### Part 1: Implementation\n",
    "\n",
    "For this assignment, I'm going to let you break down the implementation as you see fit. You're going to implement KNN. In brief, this means:\n",
    "\n",
    "- calculating euclidean distance between a test instance, and a training instance\n",
    "- iterating through all the training instances, and storing distances in a list\n",
    "- returning the k nearest neighbors\n",
    "- making a prediction by choosing the class that appears the most in the k nearest neighbors\n",
    "\n",
    "To calculate euclidean distance between an instance x1 and an instance x2, we need to iterate through the features (excluding the class) of the two instances (for i features) and for each take the difference of (x1[i]) - (x2[i]), squaring that difference, and summing over all features. At the end, take the square root of the total. In other words:\n",
    "\n",
    "$$distance=\\sqrt{\\sum_{i=1}^n (x1_{i} - x2_{i})^2}$$\n",
    "\n",
    "I didn't really need to include this equation, but now you have an example of embedding a latex equation inside markdown. You're welcome.\n",
    "\n",
    "I would strongly suggest you follow the implementation outline of previous algorithms in terms of the functions you use, but I'm leaving it up to you.\n",
    "\n",
    "Below is the same contrived dataset you've used before. If your code works, you should be able to take an instance of this data, and compare it to all the others (including itself, where the distance SHOULD be 0). You should be able to select the k-nearest neighbors, and make a prediction based on the most frequently occuring class.\n",
    "\n",
    "Make sure you create a knn function that takes a training set, a test set and a value for k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "from collections import Counter \n",
    "\n",
    "# Contrived data set\n",
    "\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "    [3.110073483,1.781539638,0],\n",
    "    [1.343808831,3.368360954,0],\n",
    "    [3.582294042,4.67917911,0],\n",
    "    [2.280362439,2.866990263,0],\n",
    "    [7.423436942,4.696522875,1],\n",
    "    [5.745051997,3.533989803,1],\n",
    "    [9.172168622,2.511101045,1],\n",
    "    [7.792783481,3.424088941,1],\n",
    "    [7.939820817,0.791637231,1]]\n",
    "\n",
    "def euclid(train_inst, test_inst):\n",
    "    inst_len = len(train_inst)-1\n",
    "    dist = 0\n",
    "    for i in range(inst_len):\n",
    "        dist += (train_inst[i] - test_inst[i])**2\n",
    "    return math.sqrt(dist)\n",
    "    \n",
    "def nearest_neighbor(train, test_inst, k):\n",
    "    dist_list = []\n",
    "    for i in range(len(train)):       \n",
    "        dist_list.append([euclid(train[i], test_inst), train[i][-1]])\n",
    "    dist_list.sort(key = lambda dist: dist[0])\n",
    "    knn = []\n",
    "    for i in range(k):\n",
    "        knn.append(dist_list[i][-1])\n",
    "    return knn\n",
    "\n",
    "def knn_class(train, test, k):\n",
    "    predict = []\n",
    "    for instance in test:\n",
    "        knn = nearest_neighbor(train, instance, k)\n",
    "        predict.append(Counter(knn).most_common(1)[0][0])\n",
    "    return predict \n",
    "\n",
    "print(knn_class(dataset, dataset, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Working with real data\n",
    "\n",
    "Apply the KNN algorithm above to the abalone data set. You can find more about it here: http://archive.ics.uci.edu/ml/datasets/Abalone\n",
    "\n",
    "You will need to make the class value into an integer class. Run a 5-fold cross-validation, with k set as 5. Also run a classification baseline. Report on classification accuracy, and write up some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of instances: 4177\n",
      "Number of features : 8 \n",
      "\n",
      "KNN highest accuracy: 27.392344497607656\n",
      "KNN lowest accuracy: 23.086124401913878\n",
      "KNN mean accuracy: 25.185212841117306 \n",
      "\n",
      "zeroRC highest accuracy: 17.3444976076555\n",
      "zeroRC lowest accuracy: 14.765906362545017\n",
      "zeroRC mean accuracy: 16.493851128968334\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import copy\n",
    "import random\n",
    "\n",
    "filename = 'abalone.csv'\n",
    "\n",
    "def cross_validation_data(dataset, folds):\n",
    "    new_list = []\n",
    "    copy_list = copy.deepcopy(dataset)\n",
    "    fold_len = len(dataset)/folds\n",
    "    for i in range(folds):\n",
    "        current_fold = []\n",
    "        while len(current_fold) < fold_len and len(copy_list)!=0:\n",
    "            random_inst = random.choice(copy_list) \n",
    "            current_fold.append(random_inst)\n",
    "            copy_list.remove(random_inst)\n",
    "        new_list.append(current_fold)\n",
    "    return new_list\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, folds, metric, *args):\n",
    "    new_data = cross_validation_data(dataset, folds)  \n",
    "    scores = []\n",
    "    for fold in new_data:\n",
    "        train = copy.deepcopy(new_data)\n",
    "        train.remove(fold)\n",
    "        train = [element for sublist in train for element in sublist]\n",
    "        test = [instance[:-1] + [None] for instance in fold]\n",
    "        \n",
    "        predicted = algorithm(train,test, *args)\n",
    "        actual = [instance[-1] for instance in fold]\n",
    "        result = metric(actual,predicted)\n",
    "        scores.append(result)\n",
    "\n",
    "    return scores\n",
    "\n",
    "#=======================================================\n",
    "def load_data(filename):\n",
    "    csv_reader = csv.reader(open(filename, newline=''), delimiter=',')\n",
    "    new_list = []\n",
    "    for row in csv_reader:\n",
    "        new_list.append(row)\n",
    "    return new_list\n",
    "\n",
    "data_abalone = load_data(filename)\n",
    "\n",
    "# Convert the features from strings to floats \n",
    "def column2Float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column])\n",
    "        \n",
    "for row in data_abalone:\n",
    "    if row[0] == 'M':\n",
    "        row[0] = 0\n",
    "    elif row[0] == 'F':\n",
    "        row[0] = 1\n",
    "    else:\n",
    "        row[0] = 2\n",
    "        \n",
    "for i in range(1,len(data_abalone[0])):  \n",
    "    column2Float(data_abalone, i)  \n",
    "\n",
    "def mean(listOfValues):\n",
    "    return sum(listOfValues)/len(listOfValues)\n",
    "\n",
    "#==============================================\n",
    "def accuracy(actual, predicted):\n",
    "    length = len(actual)\n",
    "    counter = 0\n",
    "    for i in range(length):\n",
    "        if actual[i] == predicted[i]:\n",
    "            counter += 1\n",
    "    return (counter/length)*100\n",
    "\n",
    "def zeroRC(train, test):\n",
    "    valueY = [instance[-1] for instance in train]\n",
    "    most_occur = Counter(valueY).most_common(1)[0][0] \n",
    "    return [most_occur for i in range(len(test))]\n",
    "\n",
    "#==============================================\n",
    "folds = 5\n",
    "k = 10\n",
    "\n",
    "KNN_scores = evaluate_algorithm(data_abalone, knn_class, folds, accuracy, k)\n",
    "zeroRC_scores = evaluate_algorithm(data_abalone, zeroRC, folds, accuracy)\n",
    "\n",
    "print(\"\\nNumber of instances:\", len(data_abalone))\n",
    "print(\"Number of features :\", len(data_abalone[0])-1, \"\\n\")\n",
    "\n",
    "print(\"KNN highest accuracy:\", max(KNN_scores))\n",
    "print(\"KNN lowest accuracy:\", min(KNN_scores))\n",
    "print(\"KNN mean accuracy:\", mean(KNN_scores), \"\\n\")\n",
    "\n",
    "print(\"zeroRC highest accuracy:\", max(zeroRC_scores))\n",
    "print(\"zeroRC lowest accuracy:\", min(zeroRC_scores))\n",
    "print(\"zeroRC mean accuracy:\", mean(zeroRC_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write up results here\n",
    "\n",
    "- KNN performs better than the baseline, however, the accuracy rate is not very high. This may be due to the fact that the class (Age) varies widely (1-29 yrs old) and number of neighbors (k=5) is not enough to capture the right class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: KNN regression\n",
    "\n",
    "We can also run KNN as a regression algorithm. In this case, instead of predicting the most common class in the k nearest neighbors, we can assign a predicted value that is the mean of the values in the k neighbors. \n",
    "\n",
    "Make this change to your algorithm (presumably by simply implementing a new predict function below, because you divided your code up sensibly in Part 1), and run the abalone data as a regression problem (convert the class data to a float, before running the algorithm). Use the same number of folds and the same k value as before. Also run a regression baseline and report RMSE values for both. Give me some explanation of the results, both standalone and in comparison to the classification results above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of instances: 4177\n",
      "Number of features : 8 \n",
      "\n",
      "KNN highest error: 2.352113983502406\n",
      "KNN lowest error: 2.1812911801235964\n",
      "KNN mean error: 2.2642945227933895 \n",
      "\n",
      "zeroRR highest error: 3.3405832728341984\n",
      "zeroRR lowest error: 3.089706040475734\n",
      "zeroRR mean error: 3.2234056127149393\n"
     ]
    }
   ],
   "source": [
    "def knn_reg(train, test, k):\n",
    "    predict = []\n",
    "    for instance in test:\n",
    "        knn = nearest_neighbor(train, instance, k)\n",
    "        predict.append(mean(knn))\n",
    "    return predict \n",
    "\n",
    "def zeroRR(train, test):\n",
    "    targetList = [instance[len(instance)-1] for instance in train]\n",
    "    targetMean = mean(targetList)\n",
    "    prediction = [targetMean for i in range(len(train))]\n",
    "    return prediction\n",
    "        \n",
    "def rmse_eval(actual, predicted):\n",
    "    error = 0.0\n",
    "    length = len(actual)\n",
    "    for i in range(length):\n",
    "        error += (predicted[i] - actual[i])**2\n",
    "    error = (error/length)**0.5\n",
    "    return error\n",
    "\n",
    "#==============================================\n",
    "data_abalone = load_data(filename)\n",
    "\n",
    "for row in data_abalone:\n",
    "    if row[0] == 'M':\n",
    "        row[0] = 0\n",
    "    elif row[0] == 'F':\n",
    "        row[0] = 1\n",
    "    else:\n",
    "        row[0] = 2\n",
    "        \n",
    "for i in range(len(data_abalone[0])):  \n",
    "    column2Float(data_abalone, i)\n",
    "\n",
    "folds = 5\n",
    "k = 5\n",
    "\n",
    "KNN_scores = evaluate_algorithm(data_abalone, knn_reg, folds, rmse_eval, k)\n",
    "zeroRR_scores = evaluate_algorithm(data_abalone, zeroRR, folds, rmse_eval)\n",
    "\n",
    "print(\"\\nNumber of instances:\", len(data_abalone))\n",
    "print(\"Number of features :\", len(data_abalone[0])-1, \"\\n\")\n",
    "\n",
    "print(\"KNN highest error:\", max(KNN_scores))\n",
    "print(\"KNN lowest error:\", min(KNN_scores))\n",
    "print(\"KNN mean error:\", mean(KNN_scores), \"\\n\")\n",
    "\n",
    "print(\"zeroRR highest error:\", max(zeroRR_scores))\n",
    "print(\"zeroRR lowest error:\", min(zeroRR_scores))\n",
    "print(\"zeroRR mean error:\", mean(zeroRR_scores))\n",
    "\n",
    "\n",
    "def arrangingRules(rules):\n",
    "    # Write your code here\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fptr = open(os.environ['OUTPUT_PATH'], 'w')\n",
    "\n",
    "    rules_count = int(input().strip())\n",
    "\n",
    "    rules = []\n",
    "\n",
    "    for _ in range(rules_count):\n",
    "        rules_item = input()\n",
    "        rules.append(rules_item)\n",
    "\n",
    "    result = arrangingRules(rules)\n",
    "\n",
    "    fptr.write('\\n'.join(result))\n",
    "    fptr.write('\\n')\n",
    "\n",
    "    fptr.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write up results here\n",
    "\n",
    "KNN performs nicely with error rate around 2. However, this is not much higher than that of zeroRR which may be due to the fact that most inputs have age clusters around the mean (age 6-11 make up 3028 instances, 3/4 of the dataset).\n",
    "\n",
    "This method of assigning mean values works better than the previous one which attempts to make exact prediction. Again, this is because age varies in a wide range, making it hard to predict the exact age but easier to predict a value that is close enough to the true one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
